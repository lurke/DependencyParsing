\documentclass[12pt,fleqn]{amsart}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{setspace}

\onehalfspacing

\title{CS 187 - Dependency Parsing\\Phase 4: Final Paper Draft}
\author{Lauren Urke, Nathaniel Herman, Henrik Sigstad, Ariel Camperi}
\date{}

\begin{document}
    \begin{abstract}
    Utterance structure parsing is a very important area of computational linguistics, as it is a precursor to meaning analyses of utterances. Dependency parsing is one such method which parses an utterance into a structure based on inter-word dependency relationships. This paper presents a method for using Support Vector Machines to deterministically generate this dependency tree.
    \end{abstract}

    \maketitle

\section{Introduction}
The paper we were assigned proposes that texts in specialized domains such as medical or legal documents are more amenable to word-word dependency analysis than to phrase structure analysis. This is because generating accurate phrase structure parses in these domains requires training models on annotated phrase structure parses of corpora in the relevant domain. This is hard for experts in the relevant domains to do, as this it necessitates rather deep linguistic knowledge. Therefore it is much more realistic to have such domain-experts annotate corpora with word-word dependencies, which require only superficial linguistic knowledge.

The method we present in this paper uses Support Vector Machines (described more in detail in further sections) to train a model from annotated Penn treebank data, using a variety of local features for target word pairings. This model is then applied in a bottom-up manner to ``flat'' sentences (i.e. un-annotated with structure) and used to generate a dependency tree. The motivation for this is to see whether a model trained using only word-word dependency data in a given domain is good enough to predict word-word dependencies on an unknown text in that domain.

The paper is organized as follows. First we will describe the method we used in more detail, including an explanation of how we used Support Vector Machines and why they were relevant to the problem, and a explanation of the attached source code. Next we desribe the data that was used to train and test this method. Finally, our results are presented and we draw conclusions from them.

\section{Description of the Method}

\subsection{Description of SVMs}
\subsection{Description of Dependency Parsing}
\subsection{Description of Code}

\section{Description of Data}
To implement Dependency Parsing, we used the Penn Treebank Project Data. The treebank data contains sentences that have been parsed into a linguistic trees that also contain part-of-speech tagging. \\

Parsed in this way, "Pieere Vinken, 61 years old, will join the board as a non executive director Nov. 29.", will look like this: 
\begin{verbatim}
( (S
         (NP-SBJ 
               (NP (NNP Pierre) (NNP Vinken) )
               (, ,) 
               (ADJP
                       (NP (CD 61) (NNS years) )
                       (JJ old) )
                (, ,) )
         (VP (MD will) 
               (VP (VB join) 
                       (NP (DT the) (NN board) )
                       (PP-CLR (IN as) 
                             (NP (DT a) (JJ nonexecutive) (NN director) ))
                       (NP-TMP (NNP Nov.) (CD 29) )))
         (. .) ))\end{verbatim}

This information is helpful, but could be improved. To conduct analysis on the dependencies, we wanted to be able to easily infer the relationship between two words. Therefore, we used a program from the Lund University Computer Science Department that converted the Penn Treebank parses into dependency parses. For each word in a sentence, a dependency parse indicates the parent word and the part of speech. For example, the same sentence now comes out as this:
\\

        \begin{tabular}{|ccccc|}
            ID & Token & Part of Speech & Parent ID & Class \\ 
            \hline
            1 &Pierre & NNP & 2 & NAME \\
            2 & Vinken & NNP & 8 & SBJ\\
            3 & , & , & 2 & P\\
            4 & 61 & CD & 5 & NMOD\\
            5 & years & NNS & 6 & AMOD\\
            6 & old & JJ & 2 & APPO\\
            7 & , & , & 2 & P\\
            8 & will & MD & 0 & ROOT\\
            9 & join & VB & 8 & VC\\
            10 & the & DT & 11 & NMOD\\
            11 & board & NN & 9 & OBJ\\
            12 & as & IN & 9 & ADV\\
            13 & a & DT & 15 & NMOD\\
            14 & nonexecutive & JJ & 15 & NMOD\\
            15 & director & NN & 12 & PMOD\\
            16 & Nov. & NNP & 9 & TMP\\
            17 & 29 & CD & 16 & NMOD\\
            18 & . & . & 8 & P\\
            \hline
        \end{tabular} \\


This data is now much more helpful for determining whether there is a direct parental relationship between two words in a sentence. The Penn Treebank contains 24 sections. We used sections 02-22 for training and section 23 for testing. Section 23 contains 2,416 sentences, which includes 56,684 words (49,892 without punctuation).


\section{Results}
We used three measurements to evaluate the effectiveness of this method.\\

\noindent \\
Dependency Accuracy = $\frac{\text{number of correct parents}}{\text{total number of parents}}$\\
Root Accuracy = $\frac{\text{number of correct root nodes}}{\text{total number of sentences}}$\\
Complete Rate  = $\frac{\text{number of complete parsed sentences}}{\text{total number of sentences}}$\\

\noindent \\
Using a default context of (2,2), we evaluated two different linear SVC methods. With a One vs One Linear SVC method, we obtained the following results:\\
        \begin{tabular}{|c|c|c|}
            \hline Dependency Accuracy & Root Accuracy & Complete Rate \\ \hline
            0.89135 & 0.93675 & 0.32711 \\ \hline
        \end{tabular}\\

\noindent \\
Using a One vs Many Linear SVC, we obtained the following, slightly worse results: \\
       \begin{tabular}{|c|c|c|}
            \hline Dependency Accuracy & Root Accuracy & Complete Rate \\ \hline
            0.88680 & 0.93894 & 0.32285 \\ \hline
        \end{tabular} \\
        
\noindent \\        
The Sci-kit Learn offered several other SVM options, but after testing each of them with a set context length, we determined
that none of them improved our results. We then empirically determined the optimal context length for the One vs One classifier. The Context determines how many words to the left and right of the target nodes are considered. \\
\begin{tabular}{|l|cccc|cccc|}
            \cline{2-9} \multicolumn{1}{c|}{} & (2,2) & (2,3) & (2,4) & (2,5) & (3,2) & (3,3) & (3,4) & (3,5) \\ \hline
            Dep. Acc. & 0.891 & 0.890 & 0.890 & 0.889 & 0.887 & 0.887 & 0.887 & 0.887 \\
            Root Acc. & 0.937 & 0.928 & 0.934 & 0.930 & 0.932 & 0.929 & 0.928 & 0.927 \\
            Comp. Rate & 0.327 & 0.330 & 0.325 & 0.326 & 0.312 & 0.318 & 0.316 & 0.317 \\ \hline
\end{tabular}\\

\noindent \\
We determined that the One vs One Linear SVC gave the best results when used with a context of 2 on either side of the target nodes. 

\section{Conclusion}

\noindent \\
Yamada and Matsumoto's implementation of dependency parsing used a One vs One classifier to parse the sentences. They tested different kernel degrees, and ended up with a quadratic kernel. They also determined that a context of 2 on either side of the target nodes was best. Their quadratic implementation gave only slightly better results than our linear implementation, while their linear implementation did worse than ours:       \\
 \begin{tabular}{|c|c|c|c|}
            \hline & Dependency Accuracy & Root Accuracy & Complete Rate \\ \hline
            Y and M Quadratic & 0.900 & 0.896 & 0.379 \\ \hline
            Y and M Linear & 0.854 & 0.811 & 0.261 \\ \hline
            Our Linear Results & 0.89135 & 0.93675 & 0.32711 \\ \hline

        \end{tabular}\\

Our implementation successfully modeled Yamada and Matsumoto's dependency parsing algorithm with fairly similar results on the linear kernel level. In theory, a One vs Many classifier should work better than a three-part One vs One classifier. Possible next steps to improve results include investigating other One vs Many classifiers and attempting to pinpoint why this classifier did not improve parsing accuracy. Another possible next step includes testing out various other kernel functions. \\
        
        


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
