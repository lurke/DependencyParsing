\documentclass[12pt,fleqn]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{enumitem}

\title{CS 187 - Dependency Parsing\\Phase 4: Final Paper Draft}
\author{Lauren Urke, Nathaniel Herman, Henrik Sigstad, Ariel Camperi}
\date{}

\begin{document}
    \maketitle
    \hrule
\begin{abstract}
The abstract ...
\end{abstract}
\section{Introduction}
\begin{itemize}
\item Motivation
\item What we did and why we think it is important
\item Statement of key result
\end{itemize}

\section{Statement of the problem}
\begin{itemize}
\item Further discussion about why we care...
\end{itemize}

\section{Description of the method}

\subsection{Description of SVMs}
\subsection{Description of dependency parsing}
\subsection{Description of code}

\section{Description of data}
To implement Dependency Parsing, we used the Penn Treebank Project Data. The treebank data contains sentences that have been parsed into a linguistic trees that also contain part-of-speech tagging. \\

Parsed in this way, "Pieere Vinken, 61 years old, will join the board as a non executive director Nov. 29.", will look like this: 

NEED TO FIGURE OUT HOW TO TAB THIS
\begin{tabbing}
( (S\\ 
 	 (NP-SBJ \\
		(NP (NNP Pierre) (NNP Vinken) )\\
		(, ,) \\
		(ADJP\\ 
			(NP (CD 61) (NNS years) )\\
			(JJ old) )\\
		(, ,) )\\
	(VP (MD will) \\
		(VP (VB join) \\
			(NP (DT the) (NN board) )\\
			(PP-CLR (IN as) \\
				(NP (DT a) (JJ nonexecutive) (NN director) ))\\
			(NP-TMP (NNP Nov.) (CD 29) )))\\
	(. .) ))
	\\
\end{tabbing}

This information is helpful, but could be improved. To conduct analysis on the dependencies, we wanted to be able to easily infer the relationship between two words. Therefore, we used a program from the Lund University Computer Science Department that converted the Penn Treebank parses into dependency parses. For each word in a sentence, a dependency parse indicates the parent word and the part of speech. For example, the same sentence now comes out as this:
\\

        \begin{tabular}{||ccccc|}
            ID & Token & Part of Speech & Parent ID & Class \\ 
            \hline
            1 &Pierre & NNP & 2 & NAME \\
            2 & Vinken & NNP & 8 & SBJ\\
            3 & , & , & 2 & P\\
            4 & 61 & CD & 5 & NMOD\\
            5 & years & NNS & 6 & AMOD\\
            6 & old & JJ & 2 & APPO\\
            7 & , & , & 2 & P\\
            8 & will & MD & 0 & ROOT\\
            9 & join & VB & 8 & VC\\
            10 & the & DT & 11 & NMOD\\
            11 & board & NN & 9 & OBJ\\
            12 & as & IN & 9 & ADV\\
            13 & a & DT & 15 & NMOD\\
            14 & nonexecutive & JJ & 15 & NMOD\\
            15 & director & NN & 12 & PMOD\\
            16 & Nov. & NNP & 9 & TMP\\
            17 & 29 & CD & 16 & NMOD\\
            18 & . & . & 8 & P\\
            \hline
        \end{tabular} \\


This data is now much more helpful for determining whether there is a direct parental relationship between two words in a sentence. The Penn Treebank contains 24 sections. We used sections 02-22 for training and section 23 for testing. Section 23 contains 2,416 sentences, which includes 56,684 words (49,892 without punctuation).


\section{Results}
\subsection{Results}
We used three measurements to evaluate the effectiveness of this method.\\

\noindent \\
Dependency Accuracy = $\frac{\text{number of correct parents}}{\text{total number of parents}}$\\
Root Accuracy = $\frac{\text{number of correct root nodes}}{\text{total number of sentences}}$\\
Complete Rate  = $\frac{\text{number of complete parsed sentences}}{\text{total number of sentences}}$\\

\noindent \\
Using a default context of (2,2), we evaluated two different SVC methods. With a One vs One Linear SVC method, we obtained the following results:\\
        \begin{tabular}{|c|c|c|}
            \hline Dependency Accuracy & Root Accuracy & Complete Rate \\ \hline
            0.89135 & 0.93675 & 0.32711 \\ \hline
        \end{tabular}\\

\noindent \\
Using a One vs Many Linear SVC, we obtained the following, slightly worse results: \\
       \begin{tabular}{|c|c|c|}
            \hline Dependency Accuracy & Root Accuracy & Complete Rate \\ \hline
            0.88680 & 0.93894 & 0.32285 \\ \hline
        \end{tabular} \\
        
\noindent \\        
The Sci-kit Learn offered several other SVM options, but after testing each of them with a set context length, we determined
that none of them improved our results. We then empirically determined the optimal context length for the One vs One classifier. The Context determines how many words to the left and right of the target nodes are considered. \\
\begin{tabular}{|l|cccc|cccc|}
            \cline{2-9} \multicolumn{1}{c|}{} & (2,2) & (2,3) & (2,4) & (2,5) & (3,2) & (3,3) & (3,4) & (3,5) \\ \hline
            Dep. Acc. & 0.891 & 0.890 & 0.890 & 0.889 & 0.887 & 0.887 & 0.887 & 0.887 \\
            Root Acc. & 0.937 & 0.928 & 0.934 & 0.930 & 0.932 & 0.929 & 0.928 & 0.927 \\
            Comp. Rate & 0.327 & 0.330 & 0.325 & 0.326 & 0.312 & 0.318 & 0.316 & 0.317 \\ \hline
\end{tabular}\\

\noindent \\
We determined that the One vs One Linear SVC gave the best results when used with a context of 2 on either side of the target nodes. 

\section{Conclusion}

\noindent \\
Yamada and Matsumoto's implementation of dependency parsing used a One vs One classifier to parse the sentences. They also determined that a context of 2 on either side of the target nodes was best. Their implementation gave only slightly better results:       \\
 \begin{tabular}{|c|c|c|c|}
            \hline & Dependency Accuracy & Root Accuracy & Complete Rate \\ \hline
            Yamada and Matsumoto & 0.900 & 0.896 & 0.379 \\ \hline
            Our results & 0.89135 & 0.93675 & 0.32711 \\ \hline

        \end{tabular}\\

Our implementation successfully modeled Yamada and Matsumoto's dependency parsing algorithm with fairly similar results. 
In theory, a One vs Many classifier should work better than a three-part One vs One classifier. Possible next steps to improve results include investigating other One vs Many classifiers and attempting to pinpoint why this classifier did not  improve parsing accuracy. \\
        
        


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
